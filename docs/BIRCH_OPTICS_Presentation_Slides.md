# Clustering Algorithm Presentation - Birch & OPTICS

## Slide 1: BIRCH Clustering Algorithm

### **Algorithm Overview**
- **BIRCH** = Balanced Iterative Reducing and Clustering using Hierarchies
- **Type**: Hierarchical clustering designed for large datasets
- **Key Innovation**: Memory-efficient - processes data incrementally without loading all points at once

### **How It Works**
1. **CF Tree Construction**: Builds a Clustering Feature (CF) tree in memory
   - Each node summarizes multiple data points
   - Uses threshold to control tree size
2. **Global Clustering**: Applies clustering on CF tree (not raw data)
3. **Label Assignment**: Maps original points to final clusters

### **Key Parameters**
| Parameter | Our Value | Purpose |
|-----------|-----------|---------|
| `n_clusters` | **2** | Final number of clusters desired |
| `threshold` | **0.2** | Maximum radius of subclusters (smaller = more clusters) |
| `branching_factor` | **70** | Maximum CF subclusters per node (controls tree width) |

### **Our Results (5G Network Zones)**

**Training Data (274 zones):**
- Silhouette Score: **0.256**
- Davies-Bouldin Index: **1.380** (lower is better)
- Calinski-Harabasz: **113.32**
- Found: **2 stable clusters**

**Test Data (219 zones):**
- Silhouette Score: **0.180**
- Generalization: **70.3% retention** âœ…
- Test DBI: **1.818**

### **Strengths**
âœ… **Memory efficient** - O(n) complexity, handles large datasets
âœ… **Fast** - Single-pass through data
âœ… **Good generalization** - 70.3% trainâ†’test retention
âœ… **Stable** - Produces consistent cluster counts

### **Weaknesses**
âš ï¸ **Sensitive to threshold** - Parameter tuning required
âš ï¸ **Assumes spherical clusters** - May miss complex shapes
âš ï¸ **Lower silhouette scores** - Trade-off for speed

### **Best Use Cases**
- Large datasets (millions of points)
- When memory is limited
- When speed matters more than perfect accuracy
- Data with relatively compact clusters

### **Visualization Insight**
- Both train and test show **2 clear geographic zones**
- Clusters align with physical network topology
- Consistent cluster membership across train/test split

---

## Slide 2: OPTICS Clustering Algorithm

### **Algorithm Overview**
- **OPTICS** = Ordering Points To Identify the Clustering Structure
- **Type**: Density-based clustering (DBSCAN evolution)
- **Key Innovation**: Creates reachability plot - finds clusters at ALL density levels simultaneously

### **How It Works**
1. **Core Distance**: For each point, find minimum Îµ to have min_samples neighbors
2. **Reachability Distance**: Measure how "reachable" each point is from others
3. **Ordering**: Sort points by reachability (creates reachability plot)
4. **Cluster Extraction**: Use xi or dbscan method to extract clusters from plot

### **Key Parameters**
| Parameter | Our Value | Purpose |
|-----------|-----------|---------|
| `min_samples` | **5** | Minimum neighbors for a core point |
| `max_eps` | **âˆž** (infinity) | Maximum distance to consider (âˆž = no limit) |
| `xi` | **0.2** | Steepness threshold for cluster detection (0-1) |
| `cluster_method` | **'xi'** | Algorithm to extract clusters from ordering |

### **Our Results (5G Network Zones)**

**Training Data (274 zones):**
- Silhouette Score: **0.763** (excellent on train!)
- Davies-Bouldin Index: **0.297** (very good)
- Found: **2 clusters**, but **262 outliers** (95.6%!) âš ï¸

**Test Data (219 zones):**
- Silhouette Score: **0.180** (massive drop! ðŸ“‰)
- Generalization: **Only 23.7% retention** âŒ
- Test DBI: **2.021** (poor)

### **Performance Analysis**
```
Train â†’ Test Retention: 23.7% (SEVERE OVERFITTING)
```

### **What Went Wrong?**
âŒ **Overfitting Issue**: OPTICS marked 96% of training data as outliers
   - High silhouette on train (0.763) by only clustering 12 "perfect" points
   - Test data forced into clusters â†’ poor quality (0.180)
   - **Memorization**, not learning real patterns

### **Strengths**
âœ… **No preset cluster count** - Discovers clusters automatically
âœ… **Handles varying densities** - Finds clusters of different shapes/sizes
âœ… **Flexible** - Can extract multiple clusterings from one run
âœ… **Outlier detection** - Naturally identifies noise points

### **Weaknesses (Highlighted in Our Project)**
âš ï¸ **Parameter sensitivity** - xi and min_samples greatly affect results
âš ï¸ **Overfitting risk** - Can mark too much data as outliers (our case: 96%!)
âš ï¸ **Poor generalization** - Our 23.7% retention is concerning
âš ï¸ **Computational cost** - O(nÂ²) complexity without optimizations

### **Why OPTICS Failed in Our Case**
1. **Too aggressive outlier marking**: 262/274 train points = noise
2. **Inflated metrics**: High silhouette only on 12 "cherry-picked" points
3. **Doesn't transfer**: Test data doesn't match train's noise pattern
4. **Wrong tool for this data**: 5G zones have clear structure, don't need aggressive noise filtering

### **Best Use Cases** (Not Our Scenario)
- Data with **truly noisy outliers** (e.g., sensor anomalies)
- **Variable density clusters** (e.g., galaxies in astronomy)
- When you need **multiple granularity levels**
- Exploratory analysis with reachability plots

### **Comparison with Winner (HDBSCAN)**
| Metric | OPTICS | HDBSCAN | Winner |
|--------|--------|---------|--------|
| Train Silhouette | 0.763 | 0.444 | OPTICS (deceptive!) |
| Test Silhouette | 0.180 | 0.324 | **HDBSCAN** âœ… |
| Generalization | 23.7% | 72.8% | **HDBSCAN** âœ… |
| Outliers (train) | 262 (96%) | 10 (3.6%) | **HDBSCAN** âœ… |
| Practical Use | âŒ Not usable | âœ… Production-ready | **HDBSCAN** âœ… |

### **Key Takeaway**
> "High training scores don't guarantee real-world performance. OPTICS's 0.763 train silhouette dropped to 0.180 on test - a classic overfitting case. Always validate on held-out test data!"

---

## Slide 3: Parameter Optimization Comparison

### **Optimization Methodology**

**Grid Search Approach:**
- **Birch**: 72 configurations tested (6 n_clusters Ã— 4 thresholds Ã— 3 branching_factors)
- **OPTICS**: 200 configurations tested (5 min_samples Ã— 5 max_eps Ã— 4 xi Ã— 2 methods)

### **Evaluation Metrics (3-Metric System)**
1. **Silhouette Score** (0-1, higher better): Measures cluster separation
2. **Davies-Bouldin Index** (lower better): Measures cluster compactness
3. **Calinski-Harabasz Score** (higher better): Measures cluster definition

**Combined Score Formula:**
```
CombinedScore = 0.40 Ã— Sil + 0.30 Ã— DBI_inv + 0.30 Ã— CH
(All normalized to 0-1 scale)
```

### **Optimal Parameters Found**

**Birch Winner:**
```
n_clusters=2, threshold=0.2, branching_factor=70
Train Combined Score: 0.497
Test Combined Score: 0.460
```

**OPTICS Winner (on train, failed on test):**
```
min_samples=5, max_eps=inf, xi=0.2, method='xi'
Train Combined Score: 0.895 (1st place!)
Test Combined Score: 0.331 (4th place!) âš ï¸
```

### **Final Ranking (Test Data - What Matters)**
1. ðŸ¥‡ **HDBSCAN** - 0.700 (Best generalization)
2. ðŸ¥ˆ **KMeans** - 0.548 (Simple but effective)
3. ðŸ¥‰ **Birch** - 0.460 (Efficient and stable)
4. **OPTICS** - 0.331 (Overfitted)
5. **DBSCAN** - 0.236 (Worst generalization)

---

## Talking Points for Presentation

### **For Birch Slide:**
1. "Birch is designed for **big data** - it processes data incrementally, making it memory-efficient"
2. "Our tuning found optimal threshold of **0.2** after testing 72 configurations"
3. "While silhouette scores are moderate, Birch shows **good generalization** at 70% retention"
4. "Trade-off: **Speed and efficiency** over perfect clustering quality"
5. "Best for: Production systems with **limited memory** and **large datasets**"

### **For OPTICS Slide:**
1. "OPTICS is powerful but **parameter-sensitive** - we tested 200 configurations"
2. "**Warning sign**: Training silhouette of 0.763 looked great, but **96% outliers** was a red flag"
3. "Test revealed the truth: Only **23.7% retention** - classic overfitting"
4. "Key lesson: **High train scores can be deceptive** - always validate on test data"
5. "OPTICS works better for data with **genuine noise**, not structured network zones"

### **Key Message:**
> "Through systematic hyperparameter tuning on train data and rigorous evaluation on held-out test data, we identified HDBSCAN as the optimal algorithm. While OPTICS showed impressive train scores (0.763), its poor generalization (23.7% retention) demonstrates why test validation is critical. Birch, though simpler, proved more reliable with 70% retention."

---

## Visual Recommendations

### **Birch Slide Visuals:**
- âœ… Side-by-side PCA plots (train vs test) showing 2 clusters
- âœ… Parameter sensitivity chart (threshold impact)
- âœ… Performance metrics bar chart (train vs test comparison)

### **OPTICS Slide Visuals:**
- âœ… Train vs Test silhouette comparison (dramatic 0.763â†’0.180 drop)
- âœ… Reachability plot showing outlier detection
- âœ… "Overfitting Warning" graphic (262 outliers visualization)
- âœ… Retention rate comparison chart (OPTICS 23.7% vs HDBSCAN 72.8%)

---

## Data Summary for Reference

**Dataset:**
- Train: 2,038,607 samples â†’ 274 zones (after aggregation)
- Test: 455,216 samples â†’ 219 zones (after aggregation)
- Features: 8 (latitude, longitude, latency metrics, throughput, zone averages)
- Domain: 5G network performance clustering

**Files Generated:**
- `clustering_optimized/` - Detailed Birch & OPTICS results with visualizations
- `tuning/` - Cross-model comparison (168 configurations)
- `final_comparison/` - Train/test evaluation with rankings

